---
title: "Assignment5Report"
author: "Louis Fabrice Tshimanga"
date: "12 giugno 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
```

#Problem Description  

!["**Figure 1**: representation of the problem"](C:/Users/louis/Desktop/Ass5.png)

From the Assignment 5, the paths graph is a directed "tree" from starting point S to points 1, 2; from point 1 to points 3, 4, and from point 2 to points 5, 6. Points 3, 4, 5, 6 all lead to target point H.
```{r, echo=FALSE}

parkour<-make_empty_graph(n = 8) %>%
  add_edges(c(1,2, 1,3, 2,4, 2,5, 3,6, 3,7), color = "blue") %>%
  add_edges(c(4,8, 5,8, 6,8, 7,8), color = "green") %>%
  set_edge_attr("reward", value = c(0,0,0,0,0,0,4,0,2,3)) %>%
  set.vertex.attribute("name", value=c("S", "1", "2", "3","4","5","6","H"))
plot(parkour)
```
  
The rewards are all null except for edges $\{3, H\}$, $\{5, H\}$ and $\{6, H\}$: 
$$
R(3,H)=+4\\
R(5,H)=+2\\
R(6,H)=+3\\
$$
  
### Graph Descriptive Matrix
We could describe the graph with a node-to-node matrix $G$ where the value of the element $g_{ij}$ is equal to the verse of the edge from $i$-th to $j$-th nodes (just a dispensable convention).
Note that here, node S is the first node, node H is the eighth node, and the numbered one are shifted (the second row represents edges starting from node named 1, and so forth):
$$
G=\left(\begin{array}{cc} 
0&1&1&0&0&0&0&0\\
-1&0&0&1&1&0&0&0\\
-1&0&0&0&0&1&1&0\\
0&-1&0&0&0&0&0&1\\
0&-1&0&0&0&0&0&1\\
0&0&-1&0&0&0&0&1\\
0&0&-1&0&0&0&0&1\\
0&0&0&-1&-1&-1&-1&0\\
\end{array}\right)
$$
  



### Transition dynamics Matrix
  
The case is deterministic in the sense that each action has its definite prescribed outcome, so the Transition Matrix would be a sparse matrix with the $1$'s of $t_{ij}$ in accordance with the given policy stating actions $a=i\to j$

### Policy-0
  
$\pi_0=\{0\to2,2\to5,1\to4\}$, with no need to specify actions for the set of state-nodes $\{3,4,5,6\}$ since they're not actual *decision* nodes: each of them leads to point $H$, with a different reward.

# Solutions

1. **Compute the state function $V^{\pi_0}(s) \ \  \forall s \in S$  **
 
  Given $\pi_0=\{0\to2,2\to5,1\to4\}$  
$$
V^{\pi_0}(0)=R(0, a=0\to2)+R(2, a=2\to5)+R(5,a=5\to H)=0+0+2=2\\
V^{\pi_0}(1)=R(1, a=1\to4)+R(4, a=4\to H)=0+0=0\\
V^{\pi_0}(2)=R(2, a=2\to5)+R(5,a=5\to H)=0+2=2\\
V^{\pi_0}(3)\equiv4\\
V^{\pi_0}(4)\equiv0\\
V^{\pi_0}(5)\equiv2\\
V^{\pi_0}(6)\equiv3\\
$$

2. **Improve the policy to obtain the new $\pi_1$  **

An improvement could be obtained by a single action change with $\pi_1=\{0\to2, 2\to6,1\to4\}$, in which the alternative action executable in state $2$ is taken.

3. **Compute the new state function $V^{\pi_1}(s) \ \  \forall s \in S$ **

  Under $\pi_1$:
$$
V^{\pi_1}(0)=R(0, a=0\to2)+R(2, a=2\to6)+R(6,a=6\to H)=0+0+3=3\\
V^{\pi_1}(1)=R(1, a=1\to4)+R(4, a=4\to H)=0+0=0\\
V^{\pi_1}(2)=R(2, a=2\to6)+R(5,a=5\to H)=0+2=3\\
V^{\pi_1}(3)\equiv4\\
V^{\pi_1}(4)\equiv0\\
V^{\pi_1}(5)\equiv2\\
V^{\pi_1}(6)\equiv3\\
$$

4. **Improve again the policy to obtain the new $\pi_2$ ** 

An improvement could be obtained by a single action change with $\pi_2=\{0\to2,2\to6,1\to3\}$, in which the alternative action executable in state $1$ is taken.

  Under $\pi_2$:
$$
V^{\pi_2}(0)=R(0, a=0\to2)+R(2, a=2\to6)+R(6,a=6\to H)=0+0+3=3\\
V^{\pi_2}(1)=R(1, a=1\to3)+R(3, a=3\to H)=0+4=4\\
V^{\pi_2}(2)=R(2, a=2\to6)+R(5,a=5\to H)=0+2=3\\
V^{\pi_2}(3)\equiv4\\
V^{\pi_2}(4)\equiv0\\
V^{\pi_2}(5)\equiv2\\
V^{\pi_2}(6)\equiv3\\
$$

  
    
      
        
It is quite evident that $\pi_2$ is not optimal, since state $1$ has a higher value than state $2$ and the policy doesn't contain an action pointing towards $1$, namely $a^*=0\to1$ instead of $a^{\pi_{2}}=0\to2$. Given $\pi_3=\{0\to1,0\to3,2\to6\}$, any "improvement" of the array $\{V^{\pi}(s) \ \  \forall s \in S\}$ would lead again to $\pi_3$, implying a convergence thus the optimality of the latest policy.