{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym import and environment setting\n",
    "import gym\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # make one call only\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    img.set_data(env.render(mode='rgb_array')) # data updater\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing, hyperparameters\n",
    "#TOTAL INITIALIZING\n",
    "\n",
    "#architecture and functions framework based on work on Pong, see: https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0\n",
    "\n",
    "batch_size = 10 # games played before updating the weights\n",
    "gamma = 0.99 # discounting factor for the delayed rewards\n",
    "decay_rate = 0.99 # decay rate for the RMSProp backpropagation algorithm\n",
    "num_hidden_layer_neurons = 200 # hidden layer neurons\n",
    "\n",
    "input_dimensions = 80 * 80 # \"pixels\" of image input, thus number of sensory neurons\n",
    "learning_rate = 1e-4 #learning rate of the ANN\n",
    "\n",
    "# Neural Network Setup\n",
    "\n",
    "weights = {\n",
    "  '1': np.random.randn(num_hidden_layer_neurons,input_dimensions) /np.sqrt(input_dimensions),\n",
    "  '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
    "} # the key names the layer; weights normalized.\n",
    "\n",
    "# the following is a ready-made setup for a specific backpropagation algo\n",
    "# To be used with rmsprop algorithm (courtesy from http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "expectation_g_squared = {}\n",
    "g_dict = {}\n",
    "for layer_name in weights.keys():\n",
    "    expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
    "    g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
    "    \n",
    "# This arrays are used in the application of the ANN transfer function\n",
    "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "\n",
    "def downsample(image):\n",
    "    # Alternate sample to half the resolution\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_color(image):\n",
    "    # Set to grey the RGB value\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def preprocess_observations(input_observation, prev_processed_observation, input_dimensions):\n",
    "    # convert the 210x160x3 uint8 frame into a float vector\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = downsample(processed_observation) #see above\n",
    "    processed_observation = remove_color(processed_observation) #see above\n",
    "   \n",
    "    # Flatten the matrix\n",
    "    processed_observation = processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    # frame by frame subtraction to process only changes in the game\n",
    "    if prev_processed_observation is not None:\n",
    "        input_observation = processed_observation - prev_processed_observation\n",
    "    else:\n",
    "        input_observation = np.zeros(input_dimensions)\n",
    "    # store the old frame to subtract from it the next one\n",
    "    prev_processed_observations = processed_observation\n",
    "    return input_observation, prev_processed_observations\n",
    "\n",
    "# Neurons activation functions / action potentials\n",
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1.0 + np.exp(x))\n",
    "\n",
    "def relu(vector):\n",
    "    vector[vector < 0] = 0\n",
    "    return vector\n",
    "\n",
    "def apply_neural_nets(observation_matrix, weights):\n",
    "    # matrix algebra to compute/process the image layer by layer\n",
    "    hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
    "    hidden_layer_values = relu(hidden_layer_values) \n",
    "    output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
    "    output_layer_values = sigmoid(output_layer_values) \n",
    "    return hidden_layer_values, output_layer_values\n",
    "\n",
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value <= probability:\n",
    "        return 2# means right in openai gym\n",
    "    else:\n",
    "        return 3 # means left in openai gym\n",
    "    \n",
    "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
    "    #See here: http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "    delta_L = gradient_log_p\n",
    "    dC_dw2 = np.dot(hidden_layer_values.T, delta_L).ravel()\n",
    "    delta_l2 = np.outer(delta_L, weights['2'])\n",
    "    delta_l2 = relu(delta_l2)\n",
    "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
    "    return {\n",
    "    '1': dC_dw1,\n",
    "    '2': dC_dw2\n",
    "    }\n",
    "\n",
    "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
    "    #See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\n",
    "    epsilon = 1e-5\n",
    "    for layer_name in weights.keys():\n",
    "        g = g_dict[layer_name]\n",
    "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
    "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    # Older actions are less important to the present result than newest action.\n",
    "    #This is a discounting based on how long ago they were taken\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    #discount the gradient with the normalized rewards\n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # rewards standardization helps balancing the gradient\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    if np.std(discounted_episode_rewards) != 0:\n",
    "        discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    else:\n",
    "        discounted_episode_rewards /= discounted_episode_rewards.size\n",
    "    return gradient_log_p * discounted_episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teaching to hit (big rewards for hitting, no rewards for surviving, no punishments) ---------- 1\n",
    "\n",
    "### Tentative learning: LEARNING TO HIT\n",
    "\n",
    "observation = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "episode_number = 0\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "prev_processed_observations = None\n",
    "done = False\n",
    "first = True\n",
    "lives = 5\n",
    "learning_record = []\n",
    "\n",
    "###############################################\n",
    "# Setup of arrays to store information used in the NN training\n",
    "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "episode_hidden2_layer_values = []\n",
    "env.reset()\n",
    "\n",
    "while episode_number <= 1000:\n",
    "        \n",
    "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "        hidden_layer_values, move_probability = apply_neural_nets(processed_observations, weights)\n",
    "    \n",
    "        episode_observations.append(processed_observations)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "        \n",
    "        action = choose_action(move_probability)\n",
    "        if first:\n",
    "            action = 1\n",
    "            first = False\n",
    "        observation, reward, done, info = env.step(action) # carry out the chosen action\n",
    "        \n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        if info['ale.lives'] < lives:\n",
    "            lives = info['ale.lives']\n",
    "            first = True\n",
    "        \n",
    "        reward_sum += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        fake_label = 1 if action == 2 else 0\n",
    "        loss_function_gradient = fake_label - move_probability\n",
    "        episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "        if done: # an episode, i.e. a game of 5 lives finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # Combine those values for the present episode\n",
    "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "            episode_observations = np.vstack(episode_observations)\n",
    "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "            episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "            # Adjust the gradient for the rewards, discounted\n",
    "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "            gradient = compute_gradient(\n",
    "                episode_gradient_log_ps_discounted,\n",
    "                episode_hidden_layer_values,\n",
    "                episode_observations,\n",
    "                weights\n",
    "                )\n",
    "\n",
    "            # Sum the gradient for use when we hit the batch size\n",
    "            for layer_name in gradient:\n",
    "                g_dict[layer_name] += gradient[layer_name]\n",
    "            \n",
    "            running_reward = reward_sum if running_reward is None else (running_reward+reward_sum)\n",
    "            \n",
    "            if (episode_number % batch_size) == 0:\n",
    "                update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "        \n",
    "                print('resetting env. episode reward total was %f. batch mean: %f' % (reward_sum, running_reward/batch_size))\n",
    "                print('episode number %f' %episode_number)\n",
    "                learning_record.append(running_reward/batch_size)\n",
    "                running_reward = None\n",
    "        \n",
    "        \n",
    "            episode_streaks = []        \n",
    "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] \n",
    "            observation = env.reset() # reset env\n",
    "            reward_sum = 0\n",
    "            prev_processed_observations = None\n",
    "            first = True\n",
    "            lives = 5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teaching not to miss (rewards for surviving, punishment for missing) ----------------------- 2\n",
    "\n",
    "observation = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "episode_number = 0\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "prev_processed_observations = None\n",
    "done = False\n",
    "first = True\n",
    "lives = 5\n",
    "\n",
    "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "episode_hidden2_layer_values = []\n",
    "env.reset()\n",
    "\n",
    "while episode_number <= 1000:\n",
    "        \n",
    "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "        hidden_layer_values, move_probability = apply_neural_nets(processed_observations, weights)\n",
    "    \n",
    "        episode_observations.append(processed_observations)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "        \n",
    "        action = choose_action(move_probability)\n",
    "        if first:\n",
    "            action = 1\n",
    "            first = False\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if reward > 0:\n",
    "            reward = 0\n",
    "        reward += 0.01\n",
    "        if info['ale.lives'] < lives:\n",
    "            lives = info['ale.lives']\n",
    "            reward = -1\n",
    "            first = True\n",
    "        \n",
    "        reward_sum += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        fake_label = 1 if action == 2 else 0\n",
    "        loss_function_gradient = fake_label - move_probability\n",
    "        episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "\n",
    "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "            episode_observations = np.vstack(episode_observations)\n",
    "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "            episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "            gradient = compute_gradient(\n",
    "                episode_gradient_log_ps_discounted,\n",
    "                episode_hidden_layer_values,\n",
    "                episode_observations,\n",
    "                weights\n",
    "                )\n",
    "\n",
    "            for layer_name in gradient:\n",
    "                g_dict[layer_name] += gradient[layer_name]\n",
    "            \n",
    "            running_reward = reward_sum if running_reward is None else (running_reward+reward_sum)\n",
    "        \n",
    "            if (episode_number % batch_size) == 0:\n",
    "                update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "                print('resetting env. episode reward total was %f. batch mean: %f' % (reward_sum, running_reward/batch_size))\n",
    "                print('episode number %f' %episode_number)\n",
    "                learning_record.append(running_reward/batch_size)\n",
    "                running_reward = None\n",
    "        \n",
    "        \n",
    "            episode_streaks = []        \n",
    "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "            observation = env.reset() \n",
    "            reward_sum = 0\n",
    "            previous_points = None\n",
    "            prev_processed_observations = None\n",
    "            first = True\n",
    "            lives = 5\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound tuning and refinement ----------------------------------------------------- 3 \n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 1e-5\n",
    "episode_number = 0\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "prev_processed_observations = None\n",
    "done = False\n",
    "first = True\n",
    "lives = 5\n",
    "combo = 0\n",
    "max_combo = 0\n",
    "hit = False\n",
    "episode_streaks = []\n",
    "\n",
    "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "episode_hidden2_layer_values = []\n",
    "env.reset()\n",
    "\n",
    "while episode_number <= 200:\n",
    "    processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "    hidden_layer_values, move_probability = apply_neural_nets(processed_observations, weights)\n",
    "    \n",
    "    episode_observations.append(processed_observations)\n",
    "    episode_hidden_layer_values.append(hidden_layer_values)\n",
    "        \n",
    "    action = choose_action(move_probability)\n",
    "    if first:\n",
    "        action = 1\n",
    "        first = False\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    if reward > 0 and not hit:\n",
    "        reward = 2\n",
    "        hit = True\n",
    "    elif reward > 0 and hit:\n",
    "        combo += 1\n",
    "        reward = (combo+1)*2\n",
    "    if info['ale.lives'] < lives:\n",
    "        lives = info['ale.lives']\n",
    "        reward = -1\n",
    "        if hit:\n",
    "            reward = -2\n",
    "            hit = False\n",
    "            episode_streaks.append(combo)\n",
    "            combo = 0\n",
    "        first = True\n",
    "          \n",
    "    reward_sum += reward\n",
    "    episode_rewards.append(reward)\n",
    "\n",
    "    fake_label = 1 if action == 2 else 0\n",
    "    loss_function_gradient = fake_label - move_probability\n",
    "    episode_gradient_log_ps.append(loss_function_gradient)\n",
    "    \n",
    "    if done:\n",
    "        episode_number += 1\n",
    "        \n",
    "        episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "        episode_observations = np.vstack(episode_observations)\n",
    "        episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "        episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "        episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "        gradient = compute_gradient(\n",
    "            episode_gradient_log_ps_discounted,\n",
    "            episode_hidden_layer_values,\n",
    "            episode_observations,\n",
    "            weights\n",
    "            )\n",
    "\n",
    "        for layer_name in gradient:\n",
    "            g_dict[layer_name] += gradient[layer_name]\n",
    "            \n",
    "        running_reward = reward_sum if running_reward is None else (running_reward+reward_sum)\n",
    "        \n",
    "        if episode_streaks and max_combo < max(episode_streaks):\n",
    "            max_combo = max(episode_streaks) \n",
    "            \n",
    "        if (episode_number % batch_size) == 0:\n",
    "            update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "            print('resetting env. episode reward total was %f. batch mean: %f' % (reward_sum, running_reward/batch_size))\n",
    "            print('episode number %f' %episode_number)\n",
    "            print('longest combo of current batch %f' %max_combo)\n",
    "            learning_record.append(running_reward/batch_size)\n",
    "            max_combo = 0\n",
    "            running_reward = None\n",
    "        \n",
    "        \n",
    "        episode_streaks = []        \n",
    "        episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] \n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_processed_observations = None\n",
    "        first = True\n",
    "        lives = 5\n",
    "        hit = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play\n",
    "\n",
    "# Tentative cycle POST learning\n",
    "observation = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "episode_number = 0\n",
    "prev_processed_observations = None\n",
    "done = False\n",
    "first = True\n",
    "lives = 5\n",
    "env.reset()\n",
    "\n",
    "while done == False:\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)    \n",
    "    processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "    hidden_layer_values, move_probability = apply_neural_nets(processed_observations, weights)\n",
    "    \n",
    "    episode_observations.append(processed_observations)\n",
    "    episode_hidden_layer_values.append(hidden_layer_values)\n",
    "        \n",
    "    action = choose_action(move_probability)\n",
    "    if first:\n",
    "        action = 1\n",
    "        first = False\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    if info['ale.lives'] < lives:\n",
    "            lives = info['ale.lives']\n",
    "            first = True\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistics\n",
    "tot_reward = []\n",
    "for _ in range(150):\n",
    "    observation = env.reset()\n",
    "    episode_number = 0\n",
    "    prev_processed_observations = None\n",
    "    done = False\n",
    "    first = True\n",
    "    lives = 5\n",
    "    game_reward = 0\n",
    "    env.reset()\n",
    "    while done == False:\n",
    "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "        hidden_layer_values, move_probability = apply_neural_nets(processed_observations, weights)\n",
    "    \n",
    "        episode_observations.append(processed_observations)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "        \n",
    "        action = choose_action(move_probability)\n",
    "        if first:\n",
    "            action = 1\n",
    "            first = False\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if info['ale.lives'] < lives:\n",
    "                lives = info['ale.lives']\n",
    "                first = True\n",
    "        game_reward += reward\n",
    "    tot_reward.append(game_reward)\n",
    "    \n",
    "print(np.mean(tot_reward))\n",
    "print(np.median(tot_reward))\n",
    "print(np.std(tot_reward))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
